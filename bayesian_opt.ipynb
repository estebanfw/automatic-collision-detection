{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import lightgbm as lgb\n",
                "from bayes_opt import BayesianOptimization\n",
                "from sklearn.datasets import load_boston\n",
                "from sklearn.metrics import r2_score, mean_squared_log_error, mean_squared_error,mean_absolute_error\n",
                "import datetime as dt\n",
                "import pickle\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "from sklearn.model_selection import train_test_split\n",
                "import preparing_data as F"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "data=pd.read_pickle(\"./dataframe/df_20210912_133401_ONLY_RISKY_EVENTS-6-withFE.pkl\")\n",
                "data.reset_index(inplace=True)\n",
                "data.drop(['index'], inplace=True, axis=1)\n",
                "print(data.shape)\n",
                "data.head()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#data=data.head(100)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "data.shape"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# ############################### FEATURE ENGINEERING ##########################################\n",
                "# # Gradient: Miss distance two last CDM\n",
                "# data[\"_GRADIENT_MISS_DISTANCE_34\"]=(-data.MISS_DISTANCE_3+data.MISS_DISTANCE_4)/abs(data.__time_to_tca_4-data.__time_to_tca_3)\n",
                "# # Gradient: Miss distance first and last CDM\n",
                "# data[\"_GRADIENT_MISS_DISTANCE_14\"]=(-data.MISS_DISTANCE+data.MISS_DISTANCE_4)/abs(data.__time_to_tca_4-data.__time_to_tca)\n",
                "# #Gradient: COLLISSION PROBABILITY two last CDM\n",
                "# data[\"_GRADIENT_PC_34\"]=(-data.COLLISSION_PROBABILITY_3+data.COLLISSION_PROBABILITY_4)/abs(data.__time_to_tca_4-data.__time_to_tca_3)\n",
                "# #Gradient: COLLISSION PROBABILITY first and last CDM\n",
                "# data[\"_GRADIENT_PC_14\"]=(-data.COLLISSION_PROBABILITY+data.COLLISSION_PROBABILITY_4)/abs(data.__time_to_tca_4-data.__time_to_tca)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "data.shape"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#print(list(data.columns))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# aux=data\n",
                "# data=data.loc[:, data.columns.str.endswith('4','TARGET')]\n",
                "# data[\"COLLISSION_PROBABILITY_TARGET\"]=aux.COLLISSION_PROBABILITY_TARGET\n",
                "# print(list(data.columns))\n",
                "# data.shape"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "train, test = train_test_split(data, test_size=0.30, random_state=42)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "print(\"Train dataframe dimension {} x {}\".format(train.shape[0],train.shape[1]))\n",
                "print(\"Test dataframe dimension {} x {}\".format(test.shape[0],test.shape[1]))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Y_train = train[\"COLLISSION_PROBABILITY_TARGET\"]\n",
                "# X_train= train.drop([\"COLLISSION_PROBABILITY_TARGET\"], axis=1)\n",
                "# Y_test = test[\"COLLISSION_PROBABILITY_TARGET\"]\n",
                "# X_test= test.drop([\"COLLISSION_PROBABILITY_TARGET\"], axis=1)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "Y_train = train[\"COLLISSION_PROBABILITY\"]\n",
                "X_train= train.drop([\"COLLISSION_PROBABILITY\"], axis=1)\n",
                "Y_test = test[\"COLLISSION_PROBABILITY\"]\n",
                "X_test= test.drop([\"COLLISSION_PROBABILITY\"], axis=1)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "X = X_train\n",
                "y = Y_train"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def bayesian_opt_lgbm(X, y, init_iter=3, n_iters=7, random_state=11, seed = 101, num_iterations = 100):\n",
                "      dtrain = lgb.Dataset(data=X, label=y)\n",
                "      #Metric evaluation functions\n",
                "      def lgb_r2_score(preds, dtrain):                #R2\n",
                "            labels = dtrain.get_label()\n",
                "            return 'metric', r2_score(labels, preds), True\n",
                "      def lgb_root_squared_error(preds, dtrain):      #RMSE\n",
                "            labels = dtrain.get_label()\n",
                "            return 'metric', mean_squared_error(labels, preds,squared=False), True\n",
                "      def lgb_mean_absolute_error(preds, dtrain):     #MAE\n",
                "            labels = dtrain.get_label()\n",
                "            return 'metric', mean_absolute_error(labels, preds), True\n",
                "      def lgb_adjusted_r2_score(preds, dtrain):       #ADJUSTED R2\n",
                "            labels = dtrain.get_label()\n",
                "            n=dtrain.num_data()\n",
                "            k=dtrain.num_feature()\n",
                "            return 'metric', ((1-r2_score(labels, preds))*(n-1))/(n-k-1), True\n",
                "            \n",
                "      # Select metric\n",
                "      metric='lgb_r2_score'\n",
                "      metric_feval=lgb_r2_score\n",
                "\n",
                "      # Objective Function\n",
                "      def hyp_lgbm(num_leaves, feature_fraction, learning_rate, bagging_fraction, max_depth, min_split_gain, min_child_weight):\n",
                "              params = {      'application':'regression',\n",
                "                              'num_iterations': num_iterations,\n",
                "                              'early_stopping_round':50,\n",
                "                              'verbose':-1,\n",
                "                              'metric':metric} # Default parameters\n",
                "              params[\"num_leaves\"] = int(round(num_leaves))\n",
                "              params[\"learning_rate\"] = learning_rate\n",
                "              params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
                "              params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
                "              params['max_depth'] = int(round(max_depth))\n",
                "              params['min_split_gain'] = min_split_gain\n",
                "              params['min_child_weight'] = min_child_weight\n",
                "              cv_results = lgb.cv(params, dtrain, nfold=5, seed=seed,categorical_feature=[], stratified=False,\n",
                "                                  verbose_eval =None, feval=metric_feval)\n",
                "              #print(cv_results)\n",
                "              return np.max(cv_results['metric-mean'])\n",
                "    \n",
                "              # Domain space-- Range of hyperparameters \n",
                "      pds = {     'num_leaves': (60, 120),\n",
                "                  'feature_fraction': (0.1, 0.9),\n",
                "                  'bagging_fraction': (0.7, 1),\n",
                "                  'max_depth': (7, 15),\n",
                "                  'learning_rate':(0.001,0.05), \n",
                "                  'min_split_gain': (0.001, 0.1),\n",
                "                  'min_child_weight': (10, 35)\n",
                "                  }\n",
                "      # Surrogate model\n",
                "      optimizer = BayesianOptimization(hyp_lgbm, pds, random_state=random_state)\n",
                "                                          \n",
                "      # Optimize\n",
                "      optimizer.maximize(init_points=init_iter, n_iter=n_iters)\n",
                "\n",
                "      filename=\"./opt_parameters_bo/opt_parameters_df_singles_{}_{}_RISKY_EVENTS-6_FE.pkl\".format(dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),metric)\n",
                "      a_file = open(filename, \"wb\")\n",
                "      pickle.dump(optimizer.max['params'], a_file)\n",
                "      a_file.close()\n",
                "\n",
                "      return optimizer\n",
                "\n",
                "bayesian_ouput=bayesian_opt_lgbm(X, y, init_iter=5, n_iters=500, random_state=77, seed = 101,num_iterations=300)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# improving function\n",
                "def bayesian_opt_lgbm(X, y, init_iter=3, n_iters=7, random_state=11, seed = 101, num_iterations = 100,evalm=\"lgb_r2\"):\n",
                "      dtrain = lgb.Dataset(data=X, label=y)\n",
                "      #Metric evaluation functions\n",
                "      def lgb_r2(preds, dtrain):                #R2\n",
                "            labels = dtrain.get_label()\n",
                "            return 'metric', r2_score(labels, preds), True\n",
                "      def lgb_rmse(preds, dtrain):      #RMSE\n",
                "            labels = dtrain.get_label()\n",
                "            return 'metric', mean_squared_error(labels, preds,squared=False), True\n",
                "      def lgb_mae(preds, dtrain):     #MAE\n",
                "            labels = dtrain.get_label()\n",
                "            return 'metric', mean_absolute_error(labels, preds), True\n",
                "      def lgb_adjusted_r2(preds, dtrain):       #ADJUSTED R2\n",
                "            labels = dtrain.get_label()\n",
                "            n=dtrain.num_data()\n",
                "            k=dtrain.num_feature()\n",
                "            return 'metric', ((1-r2_score(labels, preds))*(n-1))/(n-k-1), True\n",
                "\n",
                "\n",
                "      metrics_dict= {   \"lgb_r2\" : lgb_r2,\n",
                "                        \"lgb_rmse\":lgb_rmse,\n",
                "                        \"lgb_mae\":lgb_mae,\n",
                "                        \"lgb_adjusted_r2\": lgb_adjusted_r2\n",
                "                        }\n",
                "      # Select metric\n",
                "      metric=str(evalm)\n",
                "      metric_feval=metrics_dict.get(str(evalm))\n",
                "\n",
                "      # Objective Function\n",
                "      def hyp_lgbm(num_leaves, feature_fraction, learning_rate, bagging_fraction, max_depth, min_split_gain, min_child_weight):\n",
                "              params = {      'application':'regression',\n",
                "                              'num_iterations': num_iterations,\n",
                "                              'early_stopping_round':50,\n",
                "                              'verbose':-1,\n",
                "                              'metric':metric} # Default parameters\n",
                "              params[\"num_leaves\"] = int(round(num_leaves))\n",
                "              params[\"learning_rate\"] = learning_rate\n",
                "              params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
                "              params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
                "              params['max_depth'] = int(round(max_depth))\n",
                "              params['min_split_gain'] = min_split_gain\n",
                "              params['min_child_weight'] = min_child_weight\n",
                "              cv_results = lgb.cv(params, dtrain, nfold=5, seed=seed,categorical_feature=[], stratified=False,\n",
                "                                  verbose_eval =None, feval=metric_feval)\n",
                "              #print(cv_results)\n",
                "              return np.max(cv_results['metric-mean'])\n",
                "    \n",
                "              # Domain space-- Range of hyperparameters \n",
                "      pds = {     'num_leaves': (60, 120),\n",
                "                  'feature_fraction': (0.1, 0.9),\n",
                "                  'bagging_fraction': (0.7, 1),\n",
                "                  'max_depth': (7, 15),\n",
                "                  'learning_rate':(0.001,0.05), \n",
                "                  'min_split_gain': (0.001, 0.1),\n",
                "                  'min_child_weight': (10, 35)\n",
                "                  }\n",
                "      # Surrogate model\n",
                "      optimizer = BayesianOptimization(hyp_lgbm, pds, random_state=random_state)\n",
                "                                          \n",
                "      # Optimize\n",
                "      optimizer.maximize(init_points=init_iter, n_iter=n_iters)\n",
                "\n",
                "      filename=\"./opt_parameters_bo/opt_parameters_df_singles_{}_{}_RISKY_EVENTS-6_FE.pkl\".format(dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),metric)\n",
                "      a_file = open(filename, \"wb\")\n",
                "      pickle.dump(optimizer.max['params'], a_file)\n",
                "      a_file.close()\n",
                "\n",
                "      return optimizer\n",
                "\n",
                "bayesian_ouput=bayesian_opt_lgbm(X, y, init_iter=5, n_iters=500, random_state=77, seed = 101,num_iterations=300,evalm=\"lgb_r2\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "metrics_dict.get(\"lgb_r2_score\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "opt_parameters=bayesian_ouput.max['params']\n",
                "opt_parameters"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# filename=\"opt_parameters_balanced_df_adjusted_{}_corr_mae_RISKY_EVENTS-8.pkl\".format(dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
                "# a_file = open(filename, \"wb\")\n",
                "\n",
                "# pickle.dump(opt_parameters, a_file)\n",
                "\n",
                "# a_file.close()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# filename=\"./opt_parameters_bo/opt_parameters_balanced_df_adjusted_20210911_215733_corr_lgb_r2_score_RISKY_EVENTS-8-lastcdm-BEST-MODEL.pkl\"\n",
                "# a_file = open(filename,\"rb\")\n",
                "# output = pickle.load(a_file)\n",
                "# opt_parameters=output\n",
                "# output"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#optimizer.max['params']"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#'bagging_fraction': 1.0, 'feature_fraction': 0.9, 'max_depth': 8.0, 'min_child_weight': 25.0, 'min_split_gain': 0.013771321931506838, 'num_leaves': 88.93816438820497}"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "hyper_params = {\n",
                "    'task': 'train',\n",
                "    'boosting_type': 'gbdt',\n",
                "    'objective': 'regression',\n",
                "    'metric': 'regression_l2',\n",
                "    'learning_rate': opt_parameters.get(\"learning_rate\"),\n",
                "    'feature_fraction': opt_parameters.get(\"feature_fraction\"),\n",
                "    'bagging_fraction': opt_parameters.get(\"bagging_fraction\"),\n",
                "    #'bagging_freq': 10,\n",
                "    'verbose': -1,\n",
                "    \"max_depth\": int(round(opt_parameters.get(\"max_depth\"))),\n",
                "    \"num_leaves\": int(round(opt_parameters.get(\"num_leaves\"))),  \n",
                "    #\"max_bin\": 512,\n",
                "    'min_split_gain' : opt_parameters.get(\"min_split_gain\"),\n",
                "    \"num_iterations\": 300,\n",
                "    \"n_estimators\": 500,\n",
                "    'min_child_weight' : opt_parameters.get(\"min_child_weight\")\n",
                "}"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#Construct a gradient boosting model.\n",
                "#gbm = lgb.LGBMRegressor(**hyper_params)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "lgbm_train = lgb.Dataset(X, label=y)\n",
                "#lgbm_eval = lgb.Dataset(X_test, label=Y_test,reference=lgbm_train)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "gbm = lgb.train(params=hyper_params,\n",
                "                train_set=lgbm_train,\n",
                "                #valid_sets=lgbm_eval,\n",
                "                #verbose_eval=20,\n",
                "                #eval_metric='lgb_r2_score',\n",
                "                #early_stopping_rounds=100\n",
                "                )"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#Build a gradient boosting model from the training set (X, y)\n",
                "\"\"\" gbm.fit(X, y,\n",
                "        eval_set=[(X_test, Y_test)],\n",
                "        eval_metric='l1',\n",
                "        early_stopping_rounds=50) \"\"\"\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "Y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# print('The r2 of prediction is:', r2_score(y, Y_pred))\n",
                "# print('The MSE of prediction is:', mean_squared_error(y, Y_pred, squared=True))\n",
                "# print('The RMSE of prediction is:', mean_squared_error(y, Y_pred, squared=False))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "print('The r2 of prediction is:', r2_score(Y_test, Y_pred))\n",
                "print('The MSE of prediction is:', mean_squared_error(Y_test, Y_pred, squared=True))\n",
                "print('The RMSE of prediction is:', mean_squared_error(Y_test, Y_pred, squared=False))\n",
                "print('The MAE of prediction is:', mean_absolute_error(Y_test, Y_pred))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# df_train = lgb.Dataset(data=X_test, label=Y_test)\n",
                "# def lgb_adjusted_r2_score(preds, dtrain):\n",
                "#     labels = dtrain.get_label()\n",
                "#     n=dtrain.num_data()\n",
                "#     k=dtrain.num_feature()\n",
                "#     return 'metric', ((1-r2_score(labels, preds))*(n-1))/(n-k-1), True"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#lgb_adjusted_r2_score(Y_pred, df_train)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Validating model"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "aux_y=pd.DataFrame(Y_test)\n",
                "aux_y.reset_index(inplace=True)\n",
                "aux_y.drop(['index'], inplace=True, axis=1)\n",
                "aux_y_pred=pd.DataFrame(Y_pred)\n",
                "aux_y_pred.reset_index(inplace=True)\n",
                "aux_y_pred.drop(['index'], inplace=True, axis=1)\n",
                "frames=[aux_y,aux_y_pred]\n",
                "result=pd.concat(frames,axis=1)\n",
                "result.columns=[\"y_true\",\"y_predicted\"]\n",
                "result[\"y_true_10\"]=10**result.y_true\n",
                "result[\"y_predicted_10\"]=10**result.y_predicted\n",
                "result"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "result[result[\"y_true_10\"]>0.00001]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "result[result[\"y_true_10\"]>0.0001]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.6 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "2e85db0d6cccfdde710fbbd04098e00f256a22b321d8c0df509f8fabaad6d9ac"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}